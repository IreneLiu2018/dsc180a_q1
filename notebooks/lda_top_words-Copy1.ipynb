{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sijieliu/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sijieliu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sijieliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('final_hdsi_faculty_updated.csv', index_col=0)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['abstract'].notna()]\n",
    "data['year'] = data['year'].astype(int)\n",
    "data = data[data['year'] >= 2015] # aaron being excluded then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert abtracts to lowercase\n",
    "\n",
    "data['abstract'] = \\\n",
    "data['abstract'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming and removing stopwords\n",
    "redundant = ['abstract', 'purpose', 'paper', 'goal']\n",
    "ss = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(ss.stem(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "data['abstract_processed'] = data['abstract'].apply(preprocess_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = CountVectorizer().fit_transform(data['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organzie author's abstracts by year\n",
    "authors = {}\n",
    "for author in data.HDSI_author.unique():\n",
    "    authors[author] = {\n",
    "        2015 : list(),\n",
    "        2016 : list(),\n",
    "        2017 : list(),\n",
    "        2018 : list(),\n",
    "        2019 : list(),\n",
    "        2020 : list(),\n",
    "        2021 : list()\n",
    "    }\n",
    "for i, row in data.iterrows():\n",
    "    authors[row['HDSI_author']][row['year']].append(row['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Yusu Wang', 'Babak Salimi', 'Arya Mazumdar', 'Berk Ustun', 'Gal Mishne', 'Mikhail Belkin', 'Tsu-Wei (Lily) Weng', 'Yian Ma', 'Zhiting Hu', 'Benjamin Smarr', 'Armin Schwartzman', 'R. Stuart Geiger', 'Arun Kumar', 'Barna Saha', 'Jingbo Shang', 'Yoav Freund', 'Alex Cloninger', 'Jelena Bradic', 'Rayan Saab', 'Mikio Aoi', 'David Danks', 'Margaret (Molly) Roberts', 'Bradley Voytek', 'Virginia De Sa', 'Rajesh Gupta', 'Dimitris Politis', 'Ilkay Altintas', 'Robin Knight', 'Shankar Subramaniam', 'Angela Yu', 'Eran Mukamel', 'Shannon Ellis', 'Henrik Christensen', 'Julian McAuley', 'Larry Smarr', 'Rose Yu', 'Vineet Bafna', 'Michael Pazzani', 'Tara Javidi', 'Young-Han Kim', 'Ery, Arias-Castro ', 'Michael Holst', 'Ronghui (Lily) Xu', 'Ruth Williams', 'Terry Sejnowski', 'Frank Wuerthwein', 'Albert Hsiao', 'Lucila Ohno-Machado', 'George Sugihara', 'Justin Eldridge'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for author, author_dict in authors.items():\n",
    "    for year, documents in author_dict.items():\n",
    "        all_docs.append(\" \".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate LDA model\n",
    "countVec = CountVectorizer()\n",
    "counts = countVec.fit_transform(all_docs)\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "model pregnanc outcom data women trial diseas dose studi clinic gestat event prednison adjust trajectori\n",
      "Topic 1:\n",
      "problem algorithm graph distribut bound approxim consid test random number propos rate base function sampl\n",
      "Topic 2:\n",
      "model estim method test sampl propos asymptot predict distribut data result base bootstrap linear signal\n",
      "Topic 3:\n",
      "microbiom microbi associ sampl studi divers sequenc communiti human data microbiota diseas method differ result\n",
      "Topic 4:\n",
      "data neuron method tree dimension graph structur topolog base imag metric space detect analysi time\n",
      "Topic 5:\n",
      "dynam causal nonlinear model seri time predict forecast structur interact stock recruit empir function approach\n",
      "Topic 6:\n",
      "model size imag effect method human function observ map gaussian statist estim scale threshold field\n",
      "Topic 7:\n",
      "model activ neural cell genom method network data result dynam human brain function signal studi\n",
      "Topic 8:\n",
      "cell type neuron gene brain regulatori singl methyl specif function express transcriptom high molecular epigenom\n",
      "Topic 9:\n",
      "model learn time applic propos data build perform train network energi improv error algorithm control\n",
      "Topic 10:\n",
      "algorithm robot data approach propos learn model base perform method result measur applic comput time\n",
      "Topic 11:\n",
      "model network neural neuron brain scale spike comput activ function dynam popul structur base learn\n",
      "Topic 12:\n",
      "workflow comput data communiti develop applic scienc perform resourc servic softwar research traffic scientif system\n",
      "Topic 13:\n",
      "usepackag code network bound scheme robust channel capac achiev problem rate general propos decod neural\n",
      "Topic 14:\n",
      "edit distanc measur cluster result preprocess differ studi symptom time string improv autism correl pregnanc\n",
      "Topic 15:\n",
      "sequenc studi sampl microbiom human microbiota data research method diseas communiti model strain provid analysi\n",
      "Topic 16:\n",
      "patient studi data diseas associ clinic compar cancer differ includ treatment result report assess model\n",
      "Topic 17:\n",
      "time algorithm data problem endotheli distanc imag flow rotat dynam identifi diffus differ studi correl\n",
      "Topic 18:\n",
      "data workflow model network learn text pattern scientif comput scale inform larg entiti dataset approach\n",
      "Topic 19:\n",
      "data model patient research method risk base dataset health result privaci develop inform share hospit\n",
      "Topic 20:\n",
      "data model method mesh result process system protein base structur simul mechan entiti diffus import\n",
      "Topic 21:\n",
      "ϕλϕμ fontan footstep footprint foothold foot fooof fool food fomit foraminifera follow follicular folk folivori\n",
      "Topic 22:\n",
      "apic clutter disregard extrema uncov unbalanc dendrit spatiotempor trial remot video neuroimag strang paley curious\n",
      "Topic 23:\n",
      "sampl microbiom communiti human data microbi studi differ microbiota method associ divers sequenc includ increas\n",
      "Topic 24:\n",
      "work data differ document task research librari discuss practic algorithm social project softwar stop help\n"
     ]
    }
   ],
   "source": [
    "# 25 topics model \n",
    "modeller = LatentDirichletAllocation(n_components=25, n_jobs=-1, random_state=123)\n",
    "result = modeller.fit_transform(counts)\n",
    "\n",
    "# display top words for each topic in the model\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "display_topics(modeller, names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-author_topic\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(25)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(all_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(result, columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add author and year\n",
    "df_document_topic['author'] = np.nan\n",
    "df_document_topic['year'] = np.nan\n",
    "df_document_topic.shape\n",
    "\n",
    "year_paper_count = {}\n",
    "for author in authors.keys():\n",
    "    if author not in year_paper_count.keys():\n",
    "        year_paper_count[author] = 0\n",
    "    year_paper_count[author] += len(authors[author])\n",
    "\n",
    "author_list = list(year_paper_count.keys())\n",
    "for i in range(0, 350, 7):\n",
    "    df_document_topic.iloc[i:i+7, 26] = author_list[i//7]\n",
    "    year = 2015\n",
    "    for j in range(i, i+7):\n",
    "        df_document_topic.iloc[j, 27] = str(year)\n",
    "        year += 1\n",
    "time_author_topic = df_document_topic\n",
    "time_author_topic.to_csv('Data/time_author_topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(modeller, open('models/agg_author_model_25_topics.pkl', 'wb'))\n",
    "pickle.dump(counts, open('models/agg_dtm.pkl', 'wb'))\n",
    "pickle.dump(countVec, open('models/agg_vectorizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
