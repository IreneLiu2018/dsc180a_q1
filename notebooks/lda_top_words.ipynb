{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sijieliu/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sijieliu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sijieliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>times_cited</th>\n",
       "      <th>concepts</th>\n",
       "      <th>journal.title</th>\n",
       "      <th>HDSI_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>[{'raw_affiliation': [], 'first_name': 'Chen',...</td>\n",
       "      <td>Elder-Rule-Staircodes for Augmented Metric Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>['space', 'metric spaces']</td>\n",
       "      <td>SIAM Journal on Applied Algebra and Geometry</td>\n",
       "      <td>Yusu Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>[{'raw_affiliation': ['Cold Spring Harbor Labo...</td>\n",
       "      <td>Semantic segmentation of microscopic neuroanat...</td>\n",
       "      <td>Understanding of neuronal circuitry at cellula...</td>\n",
       "      <td>3</td>\n",
       "      <td>['hybrid architecture', 'semantic segmentation...</td>\n",
       "      <td>Nature Machine Intelligence</td>\n",
       "      <td>Yusu Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>[{'raw_affiliation': ['MOSEK ApS, Copenhagen, ...</td>\n",
       "      <td>On homotopy types of Vietoris–Rips complexes o...</td>\n",
       "      <td>We study Vietoris–Rips complexes of metric wed...</td>\n",
       "      <td>5</td>\n",
       "      <td>['Vietoris–Rips complexes', 'wedge sum', 'metr...</td>\n",
       "      <td>Journal of Applied and Computational Topology</td>\n",
       "      <td>Yusu Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>[{'raw_affiliation': ['Computer Science and En...</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>0</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>Yusu Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>[{'raw_affiliation': [], 'first_name': 'Dingka...</td>\n",
       "      <td>Detection and skeletonization of single neuron...</td>\n",
       "      <td>Neuroscientific data analysis has traditionall...</td>\n",
       "      <td>0</td>\n",
       "      <td>['collection of neurons', 'hand-tuned paramete...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Yusu Wang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                            authors  \\\n",
       "0  2021  [{'raw_affiliation': [], 'first_name': 'Chen',...   \n",
       "1  2020  [{'raw_affiliation': ['Cold Spring Harbor Labo...   \n",
       "2  2020  [{'raw_affiliation': ['MOSEK ApS, Copenhagen, ...   \n",
       "3  2020  [{'raw_affiliation': ['Computer Science and En...   \n",
       "4  2020  [{'raw_affiliation': [], 'first_name': 'Dingka...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Elder-Rule-Staircodes for Augmented Metric Spaces   \n",
       "1  Semantic segmentation of microscopic neuroanat...   \n",
       "2  On homotopy types of Vietoris–Rips complexes o...   \n",
       "3  Detection and skeletonization of single neuron...   \n",
       "4  Detection and skeletonization of single neuron...   \n",
       "\n",
       "                                            abstract  times_cited  \\\n",
       "0                                                NaN            0   \n",
       "1  Understanding of neuronal circuitry at cellula...            3   \n",
       "2  We study Vietoris–Rips complexes of metric wed...            5   \n",
       "3  Neuroscientific data analysis has traditionall...            0   \n",
       "4  Neuroscientific data analysis has traditionall...            0   \n",
       "\n",
       "                                            concepts  \\\n",
       "0                         ['space', 'metric spaces']   \n",
       "1  ['hybrid architecture', 'semantic segmentation...   \n",
       "2  ['Vietoris–Rips complexes', 'wedge sum', 'metr...   \n",
       "3  ['collection of neurons', 'hand-tuned paramete...   \n",
       "4  ['collection of neurons', 'hand-tuned paramete...   \n",
       "\n",
       "                                   journal.title HDSI_author  \n",
       "0   SIAM Journal on Applied Algebra and Geometry   Yusu Wang  \n",
       "1                    Nature Machine Intelligence   Yusu Wang  \n",
       "2  Journal of Applied and Computational Topology   Yusu Wang  \n",
       "3                                        bioRxiv   Yusu Wang  \n",
       "4                                          arXiv   Yusu Wang  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv('final_hdsi_faculty_updated.csv', index_col=0)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['abstract'].notna()]\n",
    "data['year'] = data['year'].astype(int)\n",
    "data = data[data['year'] >= 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert abtracts to lowercase\n",
    "\n",
    "data['abstract'] = \\\n",
    "data['abstract'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming and removing stopwords\n",
    "redundant = ['abstract', 'purpose', 'paper', 'goal']\n",
    "ss = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(ss.stem(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "data['abstract_processed'] = data['abstract'].apply(preprocess_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = CountVectorizer().fit_transform(data['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organzie author's abstracts by year\n",
    "authors = {}\n",
    "for author in data.HDSI_author.unique():\n",
    "    authors[author] = {\n",
    "        2015 : list(),\n",
    "        2016 : list(),\n",
    "        2017 : list(),\n",
    "        2018 : list(),\n",
    "        2019 : list(),\n",
    "        2020 : list(),\n",
    "        2021 : list()\n",
    "    }\n",
    "for i, row in data.iterrows():\n",
    "    authors[row['HDSI_author']][row['year']].append(row['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for author, author_dict in authors.items():\n",
    "    for year, documents in author_dict.items():\n",
    "        all_docs.append(\" \".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate LDA model\n",
    "countVec = CountVectorizer()\n",
    "counts = countVec.fit_transform(all_docs)\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "measur regurgit vector quantiz finit system singular differ left multiplanar right normal stress reproduc magnitud\n",
      "Topic 1:\n",
      "microbiom microbi studi associ sampl sequenc divers human commun data diseas microbiota method differ result\n",
      "Topic 2:\n",
      "trust right autonom technolog causal provid interest vehicl moral differ propos reason valu argu reveal\n",
      "Topic 3:\n",
      "model data patient studi clinic measur outcom trial result pregnanc women estim includ treatment diseas\n",
      "Topic 4:\n",
      "approxim model langevin chemic diffus time markov chain propos coupl nois system rel background function\n",
      "Topic 5:\n",
      "erron inflat skew instanti tendenc deleteri compel neonat subcort jitter etoc multineuron overinfl psd timelin\n",
      "Topic 6:\n",
      "model adapt polici human state task predict learn decis face stop inform bayesian target behavior\n",
      "Topic 7:\n",
      "variabl femal circadian male temperatur rhythm cycl sentiment disrupt word earli dwcox theme bodi stress\n",
      "Topic 8:\n",
      "code scheme capac channel rate decod achiev bound problem gener multipl protocol region simul network\n",
      "Topic 9:\n",
      "genom patient sequenc imag cancer ecdna flow tumor method result identifi studi data base gene\n",
      "Topic 10:\n",
      "algorithm problem network bound propos optim gener compress code method graph scheme measur distribut result\n",
      "Topic 11:\n",
      "model network brain neuron neural activ dynam learn function method spike synapt comput cell state\n",
      "Topic 12:\n",
      "data patient research model health risk method base privaci result dataset hospit studi clinic develop\n",
      "Topic 13:\n",
      "estim model test method distribut sampl problem propos asymptot data consid simul bootstrap base statist\n",
      "Topic 14:\n",
      "usepackag distanc time document algorithm edit cluster problem minim approxim result improv amsbsi amsfont amssymb\n",
      "Topic 15:\n",
      "model predict distribut perturb form semant classif input scale learn ell_ short problem output evalu\n",
      "Topic 16:\n",
      "cell gene type express neuron regulatori respons singl regul data specif function brain analysi human\n",
      "Topic 17:\n",
      "data comput applic workflow time learn build research perform process system resourc develop model algorithm\n",
      "Topic 18:\n",
      "robot model learn object approach gener task present algorithm environ propos train autonom base demonstr\n",
      "Topic 19:\n",
      "data method neuron structur base graph model result point demonstr network imag gener tree scale\n",
      "Topic 20:\n",
      "model dynam method data predict time nonlinear observ effect seri function correl connect popul differ\n",
      "Topic 21:\n",
      "control time cost model search dynam strategi sar test covid rate sampl measur sleep process\n",
      "Topic 22:\n",
      "network method learn base train neural data gener algorithm epsilon provid track layer perform censorship\n",
      "Topic 23:\n",
      "neural oscil power activ measur time chang signal human featur develop brain data record cognit\n",
      "Topic 24:\n",
      "model text pattern gener data entiti propos inform user base train differ perform dataset learn\n"
     ]
    }
   ],
   "source": [
    "# 25 topics model \n",
    "modeller = LatentDirichletAllocation(n_components=25, n_jobs=-1, random_state=123)\n",
    "result = modeller.fit_transform(counts)\n",
    "\n",
    "# display top words for each topic in the model\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "display_topics(modeller, names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nj/3nflqwjj7j184fxhtqzwq7dw0000gn/T/ipykernel_9838/1259479419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Make the pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf_document_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopicnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get dominant topic for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# time-author_topic\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(25)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(all_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(result, columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(modeller, open('models\\\\agg_author_model_25_topics.pkl', 'wb'))\n",
    "pickle.dump(counts, open('models\\\\agg_dtm.pkl', 'wb'))\n",
    "pickle.dump(countVec, open('models\\\\agg_vectorizer', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
